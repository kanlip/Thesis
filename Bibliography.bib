% RMI, Original learned index
@misc{CasedLearnedIndex,
  doi = {10.48550/ARXIV.1712.01208},
  
  url = {https://arxiv.org/abs/1712.01208},
  
  author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
  
  keywords = {Databases (cs.DB), Data Structures and Algorithms (cs.DS), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Case for Learned Index Structures},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
% ALEX
@inproceedings{ALEX,
	doi = {10.1145/3318464.3389711},
  
	url = {https://doi.org/10.1145%2F3318464.3389711},
  
	year = 2020,
	month = {may},
  
	publisher = {{ACM}
},
  
	author = {Jialin Ding and Umar Farooq Minhas and Jia Yu and Chi Wang and Jaeyoung Do and Yinan Li and Hantian Zhang and Badrish Chandramouli and Johannes Gehrke and Donald Kossmann and David Lomet and Tim Kraska},
  
	title = {{ALEX}: An Updatable Adaptive Learned Index},
  
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} International Conference on Management of Data}
}
% PGM
@article{PGM, author = {Ferragina, Paolo and Vinciguerra, Giorgio}, title = {The PGM-Index: A Fully-Dynamic Compressed Learned Index with Provable Worst-Case Bounds}, year = {2020}, issue_date = {April 2020}, publisher = {VLDB Endowment}, volume = {13}, number = {8}, issn = {2150-8097}, url = {https://doi.org/10.14778/3389133.3389135}, doi = {10.14778/3389133.3389135}, abstract = {We present the first learned index that supports predecessor, range queries and updates within provably efficient time and space bounds in the worst case. In the (static) context of just predecessor and range queries these bounds turn out to be optimal. We call this learned index the Piecewise Geometric Model index (PGM-index). Its flexible design allows us to introduce three variants which are novel in the context of learned data structures. The first variant of the PGM-index is able to adapt itself to the distribution of the query operations, thus resulting in the first known distribution-aware learned index to date. The second variant exploits the repetitiveness possibly present at the level of the learned models that compose the PGM-index to further compress its succinct space footprint. The third one is a multicriteria variant of the PGM-index that efficiently auto-tunes itself in a few seconds over hundreds of millions of keys to satisfy space-time constraints which evolve over time across users, devices and applications.These theoretical achievements are supported by a large set of experimental results on known datasets which show that the fully-dynamic PGM-index improves the space occupancy of existing traditional and learned indexes by up to three orders of magnitude, while still achieving their same or even better query and update time efficiency. As an example, in the static setting of predecessor and range queries, the PGM-index matches the query performance of a cache-optimised static B+-tree within two orders of magnitude (83\texttimes{}) less space; whereas in the fully-dynamic setting, where insertions and deletions are allowed, the PGM-index improves the query and update time performance of a B+-tree by up to 71\% within three orders of magnitude (1140\texttimes{}) less space.}, journal = {Proc. VLDB Endow.}, month = {apr}, pages = {1162–1175}, numpages = {14} }

% LIPP
@misc{LIPP,
  doi = {10.48550/ARXIV.2104.05520},
  
  url = {https://arxiv.org/abs/2104.05520},
  
  author = {Wu, Jiacheng and Zhang, Yong and Chen, Shimin and Wang, Jin and Chen, Yu and Xing, Chunxiao},
  
  keywords = {Databases (cs.DB), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Updatable Learned Index with Precise Positions},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
% Gaps insertion
@misc{GapsInsertion,
  doi = {10.48550/ARXIV.2101.00808},
  
  url = {https://arxiv.org/abs/2101.00808},
  
  author = {Li, Yaliang and Chen, Daoyuan and Ding, Bolin and Zeng, Kai and Zhou, Jingren},
  
  keywords = {Databases (cs.DB), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Pluggable Learned Index Method via Sampling and Gap Insertion},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Bloom filter
@article{LearnedBloom,
  title={A model for learned bloom filters and optimizing by sandwiching},
  author={Mitzenmacher, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
% Packed Memory array
@article{PackedMemoryArray, author = {Bender, Michael A. and Hu, Haodong}, title = {An Adaptive Packed-Memory Array}, year = {2007}, issue_date = {November 2007}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {32}, number = {4}, issn = {0362-5915}, url = {https://doi.org/10.1145/1292609.1292616}, doi = {10.1145/1292609.1292616}, abstract = {The packed-memory array (PMA) is a data structure that maintains a dynamic set of N elements in sorted order in a Θ(N)-sized array. The idea is to intersperse Θ(N) empty spaces or gaps among the elements so that only a small number of elements need to be shifted around on an insert or delete. Because the elements are stored physically in sorted order in memory or on disk, the PMA can be used to support extremely efficient range queries. Specifically, the cost to scan L consecutive elements is O(1 + L/B) memory transfers.This article gives the first adaptive packed-memory array (APMA), which automatically adjusts to the input pattern. Like the traditional PMA, any pattern of updates costs only O(log2 N) amortized element moves and O(1 + (log2 N)/B) amortized memory transfers per update. However, the APMA performs even better on many common input distributions achieving only O(log N) amortized element moves and O(1+ (logN)/B) amortized memory transfers. The article analyzes sequential inserts, where the insertions are to the front of the APMA, hammer inserts, where the insertions “hammer” on one part of the APMA, random inserts, where the insertions are after random elements in the APMA, and bulk inserts, where for constant α ϵ [0, 1], Nα elements are inserted after random elements in the APMA. The article then gives simulation results that are consistent with the asymptotic bounds. For sequential insertions of roughly 1.4 million elements, the APMA has four times fewer element moves per insertion than the traditional PMA and running times that are more than seven times faster.}, journal = {ACM Trans. Database Syst.}, month = {nov}, pages = {26–es}, numpages = {43}, keywords = {sparse array, range query, cache oblivious, Adaptive packed-memory array, rebalance, locality preserving, sequential file maintenance, packed-memory array, sequential scan} }

% Flood learned multi-dimension
@inproceedings{FloodLMD,
	doi = {10.1145/3318464.3380579},
  
	url = {https://doi.org/10.1145%2F3318464.3380579},
  
	year = 2020,
	month = {may},
  
	publisher = {{ACM}
},
  
	author = {Vikram Nathan and Jialin Ding and Mohammad Alizadeh and Tim Kraska},
  
	title = {Learning Multi-Dimensional Indexes},
  
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} International Conference on Management of Data}
}
@misc{Tsunami,
  doi = {10.48550/ARXIV.2006.13282},
  
  url = {https://arxiv.org/abs/2006.13282},
  
  author = {Ding, Jialin and Nathan, Vikram and Alizadeh, Mohammad and Kraska, Tim},
  
  keywords = {Databases (cs.DB), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Tsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed Workloads},
  
  publisher = {arXiv},
  
  year = {2020},
}

@inproceedings{fittingtree,
	doi = {10.1145/3299869.3319860},
  
	url = {https://doi.org/10.1145%2F3299869.3319860},
  
	year = 2019,
	month = {jun},
  
	publisher = {{ACM}
},
  
	author = {Alex Galakatos and Michael Markovitch and Carsten Binnig and Rodrigo Fonseca and Tim Kraska},
  
	title = {{FITing}-Tree},
  
	booktitle = {Proceedings of the 2019 International Conference on Management of Data}
}
% 
@article{packedquadtree,
  title={Packed-Memory Quadtree: A cache-oblivious data structure for visual exploration of streaming spatiotemporal big data},
  author={Toss, Julio and Pahins, Cicero AL and Raffin, Bruno and Comba, Jo{\~a}o LD},
  journal={Computers \& Graphics},
  volume={76},
  pages={117--128},
  year={2018},
  publisher={Elsevier}
}

% Handle update
@inproceedings{handlingupdates, author = {Hadian, Ali and Heinis, Thomas}, title = {Considerations for Handling Updates in Learned Index Structures}, year = {2019}, isbn = {9781450368025}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3329859.3329874}, doi = {10.1145/3329859.3329874}, abstract = {Machine learned models have recently been suggested as a rival for index structures such as B-trees and hash tables. An optimized learned index potentially has a significantly smaller memory footprint compared to its algorithmic counterparts, which alleviates the relatively high computational complexity of ML models. One unexplored aspect of learned index structures, however, is handling updates to the data and hence the model. In this paper we therefore discuss updates to the data and their implications for the model. Moreover, we suggest a method for eliminating the drift - the error of learned index models caused by the updates to the index- so that the learned model can maintain its performance under higher update rates.}, booktitle = {Proceedings of the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management}, articleno = {3}, numpages = {4}, keywords = {indexing, machine learning, data management}, location = {Amsterdam, Netherlands}, series = {aiDM '19} }

% Data
 @misc{openstreetmaponaws , title={Open Street Map on AWS https://registry.opendata.aws/osm/},
 author={OpenStreetMap},
 url={https://registry.opendata.aws/osm/}, journal={OpenStreetMap on AWS - Registry of Open Data on AWS}} 
%  cuckoo
 @article{cuckoo,
  title={Cuckoo hashing},
  author={Pagh, Rasmus and Rodler, Flemming Friche},
  journal={Journal of Algorithms},
  volume={51},
  number={2},
  pages={122--144},
  year={2004},
  publisher={Elsevier}
}
% Postgressql bloomfilter
 @misc{postgresqlBloom, title={PostgresSQL Bloom filter index},
 author={Postgres},
 url={https://www.postgresql.org/docs/current/bloom.html}, journal={PostgreSQL Documentation}, year={2022}, month={Aug}} 

% CDFShop
@inproceedings{CDFShop,
author = {Marcus, Ryan and Zhang, Emily and Kraska, Tim},
title = {CDFShop: Exploring and Optimizing Learned Index Structures},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384706},
doi = {10.1145/3318464.3384706},
abstract = {Indexes are a critical component of data management applications. While tree-like structures (e.g., B-Trees) have been employed to great success, recent work suggests that index structures powered by machine learning models (learned index structures) can achieve low lookup times with a reduced memory footprint. This demonstration showcases CDFShop, a tool to explore and optimize recursive model indexes (RMIs), a type of learned index structure. This demonstration allows audience members to (1) gain an intuition about various tuning parameters of RMIs and why learned index structures can greatly accelerate search, and (2) understand how automatic optimization techniques can be used to explore space/time tradeoffs within the space of RMIs.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2789–2792},
numpages = {4},
keywords = {learned index, learned index structure, index},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}
% B+TREE FAST
    @article{FAST,
    author = {Kim, Changkyu and Chhugani, Jatin and Satish, Nadathur and Sedlar, Eric and Nguyen, Anthony D. and Kaldewey, Tim and Lee, Victor W. and Brandt, Scott A. and Dubey, Pradeep},
    title = {Designing Fast Architecture-Sensitive Tree Search on Modern Multicore/Many-Core Processors},
    year = {2011},
    issue_date = {December 2011},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {36},
    number = {4},
    issn = {0362-5915},
    url = {https://doi.org/10.1145/2043652.2043655},
    doi = {10.1145/2043652.2043655},
    abstract = {In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join, and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this article, we present FAST, an extremely fast architecture-sensitive layout of the index tree. FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and Single Instruction Multiple Data (SIMD) width of the underlying hardware. FAST eliminates the impact of memory latency, and exploits thread-level and data-level parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second for large trees of 64M elements, with even better results on smaller trees. These are 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures. We also evaluated FAST on the Intel$^tinytextregistered$ Many Integrated Core architecture (Intel$^tinytextregistered$ MIC), showing a speedup of 2.4X--3X over CPU and 1.8X--4.4X over GPU. FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64M keys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.},
    journal = {ACM Trans. Database Syst.},
    month = {dec},
    articleno = {22},
    numpages = {34},
    keywords = {compression, Tree search, multicore, single instruction multiple data (SIMD), GPU, CPU, many-core}
    }

    %   CSS tree
@article{CSSCSBTree,
author = {Rao, Jun and Ross, Kenneth A.},
title = {Making B+- Trees Cache Conscious in Main Memory},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335449},
doi = {10.1145/335191.335449},
abstract = {Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well.Our goal is to make B+-Trees as cache conscious as CSS-Trees without increasing their update cost too much. We propose a new indexing technique called “Cache Sensitive B+-Trees” (CSB+-Trees). It is a variant of B+-Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node. The rest of the children can be found by adding an offset to that address. Since only one child pointer is stored explicitly, the utilization of a cache line is high. CSB+-Trees support incremental updates in a way similar to B+-Trees.We also introduce two variants of CSB+-Trees. Segmented CSB+-Trees divide the child nodes into segments. Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node. Segmented CSB+-Trees can reduce the copying cost when there is a split since only one segment needs to be moved. Full CSB+-Trees preallocate space for the full node group and thus reduce the split cost. Our performance studies show that CSB+-Trees are useful for a wide range of applications.},
journal = {SIGMOD Rec.},
month = {may},
pages = {475–486},
numpages = {12}
}


@misc{SandWichBloomFilter,
  doi = {10.48550/ARXIV.1803.01474},
  
  url = {https://arxiv.org/abs/1803.01474},
  
  author = {Mitzenmacher, Michael},
  
  keywords = {Data Structures and Algorithms (cs.DS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Optimizing Learned Bloom Filters by Sandwiching},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{benchmarking,
  doi = {10.48550/ARXIV.2006.12804},
  
  url = {https://arxiv.org/abs/2006.12804},
  
  author = {Marcus, Ryan and Kipf, Andreas and van Renen, Alexander and Stoian, Mihail and Misra, Sanchit and Kemper, Alfons and Neumann, Thomas and Kraska, Tim},
  
  keywords = {Databases (cs.DB), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Benchmarking Learned Indexes},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{xindex,
author = {Tang, Chuzhe and Wang, Youyun and Dong, Zhiyuan and Hu, Gansen and Wang, Zhaoguo and Wang, Minjie and Chen, Haibo},
title = {XIndex: A Scalable Learned Index for Multicore Data Storage},
year = {2020},
isbn = {9781450368186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332466.3374547},
doi = {10.1145/3332466.3374547},
abstract = {We present XIndex, a concurrent ordered index designed for fast queries. Similar to a recent proposal of the learned index, XIndex uses learned models to optimize index efficiency. Comparing with the learned index, XIndex is able to effectively handle concurrent writes without affecting the query performance by leveraging fine-grained synchronization and a new compaction scheme, Two-Phase Compaction. Furthermore, XIndex adapts its structure according to run-time workload characteristics to support dynamic workload. We demonstrate the advantages of XIndex with both YCSB and TPC-C (KV), a TPC-C variant for key-value stores. XIndex achieves up to 3.2X and 4.4X performance improvement comparing with Masstree and Wormhole, respectively, on a 24-core machine, and it is open-sourced1.},
booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {308–320},
numpages = {13},
location = {San Diego, California},
series = {PPoPP '20}
}

@ARTICLE{partialMonotonic,

  author={Daniels, Hennie and Velikova, Marina},

  journal={IEEE Transactions on Neural Networks}, 

  title={Monotone and Partially Monotone Neural Networks}, 

  year={2010},

  volume={21},

  number={6},

  pages={906-917},

  doi={10.1109/TNN.2010.2044803}}

@article{APEX,
	doi = {10.14778/3494124.3494141},
  
	url = {https://doi.org/10.14778%2F3494124.3494141},
  
	year = 2021,
	month = {nov},
  
	publisher = {{VLDB} Endowment},
  
	volume = {15},
  
	number = {3},
  
	pages = {597--610},
  
	author = {Baotong Lu and Jialin Ding and Eric Lo and Umar Farooq Minhas and Tianzheng Wang},
  
	title = {{APEX}
},
  
	journal = {Proceedings of the {VLDB} Endowment}
}

@misc{ShiftTable,
  doi = {10.48550/ARXIV.2101.10457},
  
  url = {https://arxiv.org/abs/2101.10457},
  
  author = {Hadian, Ali and Heinis, Thomas},
  
  keywords = {Databases (cs.DB), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Shift-Table: A Low-latency Learned Index for Range Queries using Model Correction},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{kieseberg2019analysis,
  title={Analysis of the Internals of MySQL/InnoDB B+ Tree Index Navigation from a Forensic Perspective},
  author={Kieseberg, Peter and Schrittwieser, Sebastian and Fr{\"u}hwirt, Peter and Weippl, Edgar},
  booktitle={2019 International Conference on Software Security and Assurance (ICSSA)},
  pages={46--51},
  year={2019},
  organization={IEEE}
}

@article{comer1979ubiquitous,
  title={Ubiquitous B-tree},
  author={Comer, Douglas},
  journal={ACM Computing Surveys (CSUR)},
  volume={11},
  number={2},
  pages={121--137},
  year={1979},
  publisher={ACM New York, NY, USA}
}

@inproceedings{srinivasan1991performance,
  title={Performance of B-tree concurrency control algorithms},
  author={Srinivasan, Venkathachary and Carey, Michael J},
  booktitle={Proceedings of the 1991 ACM SIGMOD international conference on Management of data},
  pages={416--425},
  year={1991}
}

@inproceedings{lourencco2015no,
  title={No SQL in practice: A write-heavy enterprise application},
  author={Louren{\c{c}}o, Jo{\~a}o Ricardo and Abramova, Veronika and Cabral, Bruno and Bernardino, Jorge and Carreiro, Paulo and Vieira, Marco},
  booktitle={2015 IEEE International Congress on Big Data},
  pages={584--591},
  year={2015},
  organization={IEEE}
}

@inproceedings{r-tree,
  title={R-trees: A dynamic index structure for spatial searching},
  author={Guttman, Antonin},
  booktitle={Proceedings of the 1984 ACM SIGMOD international conference on Management of data},
  pages={47--57},
  year={1984}
}
@article{kdtree,
  title={Real-time kd-tree construction on graphics hardware},
  author={Zhou, Kun and Hou, Qiming and Wang, Rui and Guo, Baining},
  journal={ACM Transactions on Graphics (TOG)},
  volume={27},
  number={5},
  pages={1--11},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@article{leow2004analysis,
  title={The analysis and applications of adaptive-binning color histograms},
  author={Leow, Wee Kheng and Li, Rui},
  journal={Computer Vision and Image Understanding},
  volume={94},
  number={1-3},
  pages={67--91},
  year={2004},
  publisher={Elsevier}
}


@inproceedings{sahann2021histogram,
  title={Histogram binning revisited with a focus on human perception},
  author={Sahann, Raphael and M{\"u}ller, Torsten and Schmidt, Johanna},
  booktitle={2021 IEEE Visualization Conference (VIS)},
  pages={66--70},
  year={2021},
  organization={IEEE}
}


@article{ifb-tree,
  title={Interp olation-friendly B-tr ees: Bridging the Gap Betw een AlgorithmicandLearnedInde xes},
  author={Hadian, Ali and Heinis, Thomas},
  year={2019}
}

@misc{kipf2020radixspline,
      title={RadixSpline: A Single-Pass Learned Index}, 
      author={Andreas Kipf and Ryan Marcus and Alexander van Renen and Mihail Stoian and Alfons Kemper and Tim Kraska and Thomas Neumann},
      year={2020},
      eprint={2004.14541},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}