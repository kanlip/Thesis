\lhead{\emph{Introduction}}

\chapter{Introduction}

The exponential growth of data has presented significant challenges for efficient data storage and retrieval, especially in the field of database management. Traditionally, indexing techniques such as B-trees and hash indexes have been used to organize data and facilitate fast retrieval. However, as data volumes have grown exponentially, these traditional indexing methods are becoming less effective, leading to performance degradation and increased storage costs.

In recent years, machine learning techniques have shown great promise in improving the efficiency and effectiveness of indexing in database management systems. Machine learned index is an emerging area of research that seeks to leverage the power of machine learning algorithms to create more efficient and accurate indexing methods.

The idea behind \learnindex is to train models that can learn the patterns in the data and build indexes that are optimized for the specific characteristics of that data. The models can identify and exploit correlations and dependencies within the data to produce more efficient indexing structures. By doing so, \learnindex can improve query performance and reduce storage costs, making it a promising area of research for the database management community.

The concept of machine \learnindex was first proposed by Kraska et al. in 2018 \cite{CasedLearnedIndex}, who demonstrated that machine learned indexes could outperform traditional indexing methods in terms of query response time and storage efficiency. Since then, the field has rapidly evolved, with many researchers exploring different machine learning techniques and applications in indexing.

One of the significant advantages of \learnindex is its ability to handle high-dimensional and sparse data, which traditional indexing methods struggle with. In many real-world applications, data is often high-dimensional, and the number of features can far exceed the number of instances. Machine \learnindex can leverage the power of machine learning to create indexes that are optimized for such data, improving query performance and reducing storage costs. Another advantage of machine learned index is its ability to adapt to changing data distributions. Traditional indexing methods rely on pre-defined data structures that are optimized for specific data distributions. However, as data changes over time, these structures may no longer be optimal, leading to degraded performance. Machine learned index can adapt to changing data distributions by continually retraining the models, resulting in more effective and efficient indexing structures. 

However, the drawback of the machine learning model is that it could miss predicting the location of the index. Therefore, the \learnindex will have to perform binary search on the look-up key within a bounded range, and the min and max error bounded range will have to be kept to support binary search within these ranges. Furthermore, the original \learnindex does not support updates. \ie, insertions and deletions, which is crucial in real-world systems that contain read and write operations.

In addition to the challenges of insertion performance in Learned Index, there are also other issues that need to be addressed. For example, traditional \btree indexes often rely on cache and memory optimization techniques to improve their performance. However, machine learned indexes have different requirements, and may need to consider other factors, such as the complexity of the machine learning model.

Moreover, machine learned indexes can be sensitive to changes in the underlying data distribution, which can affect the accuracy of the index. This is particularly relevant for datasets that have a high degree of variability or are subject to significant changes over time. As a result, machine learned indexes need to be carefully designed to be robust to changes in the data distribution, and may require retraining or updating periodically to maintain their accuracy.

The base \learnindex studies have open ways to many research areas such as optimising read learned index, immutable learned index on one-dimensional data \cite{ALEX, PGM, LIPP}, and multi-dimensional read-only learned index \cite{FloodLMD, Tsunami}.

The recent studies proposed a new \learnindex that supports read and write operations \cite{ALEX,fittingtree,LIPP,PGM}. These studies introduce a new concept to leave gaps in the array for subsequent insertion. These studies use model-based insert, a machine learning model, to insert items in the array. The main difference is how they handle conflicting elements when the model tries to insert an item in the location where the index could already occupy that space in the array. When there is a conflicting element, \acrfull{alex} will shift the existing elements to the closest gap to make space for the new index. \acrfull{lipp} index creates a new leaf node when there are conflicting elements that could grow the height.

Furthermore, \acrshort{lipp} introduces ways to prune these leaf nodes to reduce the height of a tree, which will also reduce the cost of query operation. However, even though they provide strategies to improve query time, they could still suffer from poor insertion performance. These poor performances are brought from extra computation required to perform when conflict elements occur during insertion operation. 

Nevertheless, these indexes that support updates operations aim to create a fast insertion and handle conflict when necessary, but it does not try to leave an empty spaces in the array strategically to reduce the number of conflicts which in turn could lead to a better search or insertion performance\cite{ALEX,fittingtree,PGM,LIPP}.

To reduce extra computation incurred by conflicts, we propose a way to insert gaps strategically so that we save costs on the subsequent few insertions without having to do the extra computation, such as creating new nodes or shifting keys. Furthermore, it provides an efficient way to insert gaps between the existing data in the data structure so that the next insertion operation would not cause a conflict. 

The main objective of this research is to optimize the performance of dynamic learned indexes, such as  \acrshort{lipp}, and \acrshort{alex} , on one-dimensional data by strategically leaving gaps for inserting new keys. In addition to this, the proposed algorithm at the end of this research aims to reduce the computation required to expand nodes in a tree-based learned index. Expansion occurs when the gapped array reaches full capacity. Our research will also focus on optimizing the gapped array to reduce the number of conflicts that occur during insertion, thereby reducing the computation required to shift or create a new child node. By achieving these objectives, we hope to improve the overall efficiency and performance of mutable learned indexes, making them more viable for use in database management systems.
